{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9478e845-e5ca-4e6a-9f51-ffbf4942a28b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hack Environment Set Up\n",
    "========================\n",
    "Install the necessary client libraries in order to demo running arbitrary code on AI compute platform. Those steps are not needed once productionized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5066d29f-3fcb-4763-99c0-a5c5cdfbc3cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh git clone -b prototype \"https://github.com/ygong1/llm-foundry.git\" ~/llm-foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75875508-9e1e-4ddc-8552-e63d11faae7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh cd ~/llm-foundry && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc9f5ab-7c1b-4d8f-92d6-031045626656",
     "showTitle": true,
     "title": "Python Library Restart Command"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a69cdb7d-652f-4a2a-88d7-107ec91c5c00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Load the Data and Perform Data Exploration before Training\n",
    "----------------------------------------------------------\n",
    "\n",
    "This [data processing notebook](https://e2-dogfood.staging.cloud.databricks.com/?o=6051921418418893#notebook/3021853473816147/command/4183847697906984) showed the details how to load CIFAR dataset into UC table and UC volumn. \n",
    "\n",
    "At the moment, StreamDataset can only process MDS formate that's differnt from the format that spark can read. We are saving the dataset in 2 copies\n",
    "1. `main.yu_gong.cifar10_train` to be loaded by spark natively to show how to do data manipulation\n",
    "2. `/Volumes/main/yu_gong/cifar10/train/` to be loaded by StreamingDataset\n",
    "\n",
    "In the demo, we hack to convert `main.yu_gong.cifar10_train` to `/Volumes/main/yu_gong/cifar10/train/` to show the idea that eventually they are going to be the same dataset, at least from user perspective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e5e4d6-498d-4290-997d-252999b2347d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check the distribution of label\n",
    "df = spark.read.table('main.yu_gong.cifar10_train') \n",
    "distribution = df.groupBy(\"label\").count().orderBy(\"count\", ascending=False)\n",
    "display(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1723072-6d7e-4a6c-8110-4c0095ca0639",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Submit Run Remotely On a Cluster\n",
    "-------------------------------\n",
    "\n",
    "Wrap up the config and submit the remote run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9468b07e-78dd-4b9a-ad4f-b527cc46d13c",
     "showTitle": true,
     "title": "Mosaic Experiment Submission Tool"
    }
   },
   "outputs": [],
   "source": [
    "from ygong.mosaic import ScalingConfig\n",
    "from ygong.mosaic import submit\n",
    "from ygong.mosaic import TrainingConfig\n",
    "from ygong.mosaic.wsfs import WSFSIntegration\n",
    "\n",
    "parameters = { \n",
    "     \"name\": \"custom-train-demo\", \n",
    "     \"seed\": 42, \n",
    "     \"device_train_microbatch_size\": 8,\n",
    "     \"data\": \"main.yu_gong.cifar10_train\",\n",
    "     \"loggers\": {\n",
    "        \"mlflow\": {\n",
    "            \"tracking_uri\": \"databricks\",\n",
    "            \"synchronous\": False,\n",
    "            \"log_system_metrics\": True\n",
    "        }\n",
    "     }\n",
    "}\n",
    "custom_code_repo_dir = \"/Workspace/Users/yu.gong@databricks.com/.ide/custom-train-demo-4bf5c137\"\n",
    "# TODO(shitao): add a prioirty field with good default. We can show it in demo\n",
    "config = TrainingConfig(\n",
    "    name=\"custom-train-demo\", \n",
    "    entry_point=f'{custom_code_repo_dir}/src/train.py', \n",
    "    parameters=parameters)\n",
    "\n",
    "scalingConfig = ScalingConfig(gpusNum=8, poolName=\"staging-aws-us-east-1-mlserv1-gentrain1\")\n",
    "\n",
    "# DEMO NOTE:\n",
    "# This is temporary hack to mock the behavior of mounting workspace filesystem to the remote training\n",
    "# nodes. Once the workspace filesystem fusion is integrated with netphos and dblet. We will get this for free.\n",
    "wsfs = WSFSIntegration(wsfs_path=custom_code_repo_dir)\n",
    "\n",
    "run = submit(config, scalingConfig, wait_job_to_finish=True, debug=True, wsfs=wsfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118a3a59-d2f9-4318-9b92-b358f205197b",
     "showTitle": true,
     "title": "Job Completion Status Checker"
    }
   },
   "outputs": [],
   "source": [
    "from mcli import RunStatus\n",
    "if run.status == RunStatus.COMPLETED:\n",
    "  print(\"Succeeded!\")\n",
    "else:\n",
    "  raise Exception(\"Pretaining job faile\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3449456614719370,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "custom_code_submit_demo",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "mosaic2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
