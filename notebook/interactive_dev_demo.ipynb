{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hack Up the Credentials\n",
    "-----------------------\n",
    "Set the up the credentials to submit the runs to `r1z1` cluster\n",
    "\n",
    "This step is **not** needed in the production solution for Databricks Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CREDENTIALS has been preset with `conda env config vars set CREDENTIALS=\"xxs\"`\n",
    "\n",
    "# To generate CREDENTIALS for your local environment, you can use the following command:\n",
    "# data = {\n",
    "#     \"workspace_url\": \"https://dbc-04ac0685-8857.staging.cloud.databricks.com/\",\n",
    "#     \"token\": \"dapi338xx\", \n",
    "#     \"mosaic_token\": \"Y4x7xx\",\n",
    "# }\n",
    "# data = json.dumps(data)\n",
    "# credentials = base64.b64encode(data.encode('utf-8')).decode('utf-8')\n",
    "\n",
    "import os\n",
    "from ygong.mosaic import submit, _set_up_environment\n",
    "\n",
    "credentials = os.environ.get(\"CREDENTIALS\")\n",
    "_set_up_environment(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on a Single Device (CPU or GPU) Single Node\n",
    "--------------------------------------------------------\n",
    "The main goal is the make sure `trainer.fit` can run on single node single CPU successfully. It could use very small smoke test dataset and try to run the model training through a few batches. \n",
    "\n",
    "This step can be run on developers' desktop if the hardware can fit the model. Or it can be run in the notebook that attached to a remote GPU instance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'train' from '/Users/yu.gong/workspace/mosaic/custom-train-demo/src/train.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "# update to use `/Workspace/Repos/yu.gong@databricks.com/custom-train-demo/src` if running on Databricks notebook\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# always reload for local updates to take effect\n",
    "import train\n",
    "importlib.reload(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "import json\n",
    "name = \"custom-train-demo\"\n",
    "config = train.MyConfig(global_train_batch_size=4, name=name)\n",
    "train.main(config)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on a Multiple GPU Device on a Single Node\n",
    "--------------------------------------------------\n",
    "This step I would prefer the terminal approach. \n",
    "\n",
    "\n",
    "1. Manually set up the environment(e.g. clone repo, install packages) that to mimic what's going to happen for remote run. In this example, following this [guide](https://docs.databricks.com/en/repos/index.html) to add [custom-train-demo](https://e2-dogfood.staging.cloud.databricks.com/browse/folders/58238802205414?o=6051921418418893) into the workspace. \n",
    "2. Open the terminal of the attached interactive GPU instance.\n",
    "3. [optional] install the llm-foundary that contains the hacks and mimic the RunConfig.integrations. *NOT NEEDED IN PRODUCTION*\n",
    "\n",
    "```bash\n",
    "git clone -b prototype \"https://github.com/ygong1/llm-foundry.git\" llm-foundry\n",
    "cd ~/llm-foundry && pip install -e .[gpu] \n",
    "```\n",
    "4. run the distributed training command. *NOTE*: `~/llm-foundry/scripts/train/launcher.py` should be replaced as `composer` in production\n",
    "\n",
    "```bash\n",
    "cd /Workspace/Repos/yu.gong@databricks.com/custom-train-demo/\n",
    "~/llm-foundry/scripts/train/launcher.py ./src/train.py $json_str\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, there is race condition of downloading the CIFAR10 dataset in [train.py](https://e2-dogfood.staging.cloud.databricks.com/?o=6051921418418893#files/58238802205419). In the Databricks workspace notebook, we can modify the train.py logic directly to only allow local rank 0 process do the downloading and the rest just wait for the dataset to be ready, save the changes and then switch to the terminal to re-execute `~/llm-foundry/scripts/train/launcher.py ./src/train.py $json_str`.\n",
    "\n",
    "We do this iteration multiple times unit the command can run successfully for distributed training on multiple GPUs on the single node.\n",
    "\n",
    "Commit the code to the repo from the workspace and push to origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"custom-train-demo\", \"seed\": 42, \"dist_timeout\": 600.0, \"global_train_batch_size\": 4, \"device_train_microbatch_size\": 16}\n"
     ]
    }
   ],
   "source": [
    "config = train.MyConfig(global_train_batch_size=4, name=name)\n",
    "json_str = json.dumps(asdict(config))\n",
    "print(json_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit Run Remotely On a Cluster\n",
    "-------------------------------\n",
    "\n",
    "Wrap up the config and submit the remote run.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcli import RunConfig\n",
    "from ygong.mosaic import ScalingConfig\n",
    "from ygong.mosaic import submit\n",
    "\n",
    "config = train.MyConfig(global_train_batch_size=4, name=name)\n",
    "json_str = json.dumps(asdict(config))\n",
    "commands = [\n",
    "    \"cd ~/custom-train-demo\",\n",
    "    f'''~/llm-foundry/scripts/train/launcher.py ./src/train.py  '{json_str}' '''\n",
    "]\n",
    "\n",
    "scalingConfig = ScalingConfig(gpusNum=8, gpuType=\"a100_80gb\", poolName=\"r1z1\")\n",
    "\n",
    "# Ideally, the following should be something like the following: then all the\n",
    "# file local changes can be automatically synced to the remote machine for training\n",
    "# integrations = [ \n",
    "#     {  \n",
    "#         'integration_type': 'databricks_repo',\n",
    "#         'git_repo': '/Workspace/Repos/yu.gong@databricks.com/custom-train-demo/',\n",
    "#     }\n",
    "# ]\n",
    "interactions = [\n",
    "   {\n",
    "      'integration_type': 'git_repo',\n",
    "      'git_repo': 'ygong1/llm-foundry',\n",
    "      'path': '~/llm-foundry',\n",
    "      'git_branch': 'prototype',\n",
    "      'pip_install': '-e .[gpu]',\n",
    "      'ssh_clone': False\n",
    "   },\n",
    "   {\n",
    "      'integration_type': 'git_repo',\n",
    "      'git_repo': 'ygong1/custom-train-demo',\n",
    "      'path': '~/custom-train-demo',\n",
    "      'ssh_clone': False\n",
    "   },\n",
    "   {\n",
    "      'integration_type': 'pip_packages',\n",
    "      'packages': ['pynvml', 'mosaicml-streaming[databricks]'],\n",
    "   },\n",
    "]\n",
    "\n",
    "config = RunConfig(\n",
    "    name=\"custom-train-demo\",\n",
    "    image='mosaicml/llm-foundry:2.2.1_cu121_flash2-latest',\n",
    "    command=\"\\n\".join(commands),\n",
    "    compute=scalingConfig.toCompute,\n",
    "    integrations= interactions,\n",
    "    env_variables={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578f4616613049249dff6ff0af2fa2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='cancel the run', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Name</th>\n",
       "      <th>Run ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Experiment Run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>custom-train-demo-gZr3uF</td>\n",
       "      <td>ffe617bd-d07a-42d6-a830-36edf56aa6d3</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-24 17:23:45,166 - ygong.mosaic.submit - DEBUG - waiting for the MLFLow experiment run to be ready, run statusRUNNING\n",
      "2024-03-24 17:23:45,166 - ygong.mosaic.submit - DEBUG - waiting for the MLFLow experiment run to be ready, run statusRUNNING\n",
      "2024-03-24 17:23:45,166 - ygong.mosaic.submit - DEBUG - waiting for the MLFLow experiment run to be ready, run statusRUNNING\n",
      "2024-03-24 17:23:45,166 - ygong.mosaic.submit - DEBUG - waiting for the MLFLow experiment run to be ready, run statusRUNNING\n",
      "2024-03-24 17:23:45,166 - ygong.mosaic.submit - DEBUG - waiting for the MLFLow experiment run to be ready, run statusRUNNING\n",
      "2024-03-24 17:23:45,166 - ygong.mosaic.submit - DEBUG - waiting for the MLFLow experiment run to be ready, run statusRUNNING\n",
      "2024-03-24 17:23:45,166 - ygong.mosaic.submit - DEBUG - waiting for the MLFLow experiment run to be ready, run statusRUNNING\n",
      "2024-03-24 17:23:45,166 - ygong.mosaic.submit - DEBUG - waiting for the MLFLow experiment run to be ready, run statusRUNNING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalingConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mosaic2/lib/python3.10/site-packages/ygong/mosaic/submit.py:204\u001b[0m, in \u001b[0;36msubmit\u001b[0;34m(model, config, scalingConfig, sync, debug)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m try_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m    203\u001b[0m     try_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 204\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         run \u001b[38;5;241m=\u001b[39m get_run(run)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "submit(None, config, scalingConfig, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mosaic2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
